# Anime4K-WebGPU

University of Pennsylvania, CIS 565: GPU Programming and Architecture, Final Project

Authors (alphabetical order with equal contribution):
* Ruijun(Daniel) Zhong [LinkedIn](https://www.linkedin.com/in/daniel-z-73158b152/) | [Personal Website](https://www.danielzhongportfolio.com/)
* Tong Hu  [LinkedIn](https://www.linkedin.com/in/tong-hu-5819a122a/) 
* Yuanqi Wang [LinkedIn](https://www.linkedin.com/in/yuanqi-wang-414b26106/) | [GitHub](https://github.com/plasmas)

## Introduction
![image](https://drive.google.com/uc?export=view&id=1XpdozjclGKwBu45OJn3IVRkVP_aMSwMx)

An [Anime4K](https://github.com/bloc97/Anime4K) implementation for WebGPU, featuring video enhancements including upscaling, denoising, and deblurring. Computing is done entirely on the client side using WebGPU compute shaders. Functionality of this implementation is published as an [NPM package](https://www.npmjs.com/package/anime4k-webgpu), and can be easily incorporated into your WebGPU pipeline.

Take a look at our web demo at https://anime4k-webgpu-demo.fly.dev/ ([Source](https://github.com/Anime4KWebBoost/Anime4K-Web-Demo))

Note: your browser must support WebGPU. See this [list](https://caniuse.com/webgpu) for compatibility.

## Usage

This package contains classes that implements interface `Anime4KPipeline`. To use these classes, first install `anime4k-webgpu` package, then insert proveded pipelines in 3 lines:

```typescript
import { Anime4KPipeline, UpscaleCNNPipeline } from 'anime4k-webgpu';

// texture to be processed
const inputTexture: GPUTexture;

// +++ instantiate pipeline +++
const pipeline: Anime4KPipeline = new UpscaleCNNPipeline(device, inputTexture);

// bind output texture wherever you want e.g. render pipeline
const renderBindGroup = device.createBindGroup({
  ...
  entries: [{
    binding: 0,
    // +++ use pipeline.getOutputTexture() instead of inputTexture +++
    resource: pipeline.getOutputTexture().createView(),
  }]
});

function frame() {
  const commandEncoder: GPUCommandEncoder;

  // +++ inject commands into the encoder +++
  pipeline.pass(commandEncoder);

  // begin other render pass...
}
```

To change an adjustable parameter (e.g. deblur strength) call `Anime4KPipeline::updateParam(param: string, value: any)` and the value will be applied for the next render cycle:
```typescript
pipeline.updateParam('strength', 3.0);
```

The input texture must have usage `TEXTURE_BINDING`, and the output texture has `TEXTURE_BINDING | RENDER_ATTACHMENT | STORAGE_BINDING` to be used in render pipelines. You can also have multiple pipelines in tandem to achieve sophisticated effects.

## Performance Analysis
### Visualization Comparisons
Following comparisons are done with a 360p image as input. The image is from Anime4K repo.

**1. Compare with Denoise & Deblur effects**

After applying denoise effect, the image become smoother. The intensity sigma is set to 0.2 and the spatial sigma is set to 2. Increasing intensity sigma will make bilateral filter approximat Gaussian convolution, and increasing the spatial sigma will make the color smoother. 

For the denoise effect, deblurring strength has been calibrated to a level of 7. As deblurring strength increasing, the effect of deblur is more distinct. This deblurring process enhances the video's clarity by sharpening the image's edges. However, it's important to note that this enhancement may also inadvertently amplify aliasing effects.

<div align="center">
<img src='https://drive.google.com/uc?export=view&id=1i5m99O5aroZYqGBKVtrGgwTYZfUKiiBu')/>
<p>Figure 1. Compare with Denoise & Deblur effect</p>
</div>

**2. Compare with restore CNN and restore GAN effects**

Utilizing restoration models on the image enhances its clarity and sharpens its edges, yet it may also result in the introduction of some aliasing. The restore GAN model, being larger in size compared to the restore CNN model, excels in enhancing edge details. However, this comes with a trade-off as it tends to introduce a greater degree of aliasing. The restoration process will enhance the image's clarity without altering its dimensions.

<div align="center">
<img src='https://drive.google.com/uc?export=view&id=1TuFOhfIgvn1gBzPPoorhZOwuk0ZqzCK2')/>
<p>Figure 2. Compare with restore CNN and restore GAN</p>
</div>

**3. Compare with upscale CNN (x2) and upscale GAN (x3 & x4) and the Real-ESRGAN effects**

Utilizing upscale CNN and upscale GAN techniques results in an increase in image size as these models aim to upscale the image, enhancing its resolution. Among these models, the upscale GAN x4 stands out as the most substantial, producing an output image that is four times larger in both width and height, yielding the most impressive results among the three upscale models. The Real-ESRGAN result is generated by [webgpu-super-resolution](https://github.com/sona1111/webgpu-super-resolution). We have upgraded the project for compatibility with the latest WebGPU standards, allowing the original image to be processed correctly. Although Real-ESRGAN delivers the sharpest image, it is significantly slowerâ€”approximately 1000 times more so. Our project is focused on achieving real-time video upscaling, which requires rapid processing. Fortunately, the upscale quality is satisfactory and well-suited for real-time video applications.

<div align="center">
<img src='https://drive.google.com/uc?export=view&id=1KSRnPYrlcZyjW2UuYM5kLAKOuUENmcV6')/>
<p>Figure 3. Compare with upscale CNN(x2) and upscale GAN (x3 & x4) and Real-ESRGAN from  </p>
</div>

**4. Compare with upscale CNN (x2) + Restore CNN and the Real-ESRGAN effects**

Input (1x):   
The original image at 200x133 pixels, serving as the starting point for upscaling.

Upscale CNN x2 + Restore CNN: Upscaling is achieved by a factor of 2 using a Convolutional Neural Network (CNN), followed by a restoration process with another CNN. This method prioritizes speed and memory efficiency, operating 1000 times faster than Real-ESRGAN and with substantially lower memory requirements, at the cost of some visual quality.

Real-ESRGAN x4:  
The Real-ESRGAN result is generated by [webgpu-super-resolution](https://github.com/sona1111/webgpu-super-resolution). This image has been upscaled by a factor of 4 using the Real-ESRGAN technique. It offers enhanced visual quality that more closely approximates the ground truth but at a cost of slower runtime and higher memory usage, which may not be feasible for real-time applications.

Ground Truth (x5):  
The reference standard with a resolution five times that of the original input, against which the other images are compared.

Conclusion:  
The comparison highlights significant trade-offs between the speed of rendering, memory usage, and the visual quality of upscaled images. While the Upscale CNN method provides a rapid and memory-efficient solution suitable for real-time applications, the Real-ESRGAN approach delivers superior image quality, which may be necessary for applications where visual fidelity is paramount, albeit with greater resource requirements.

<div align="center">
<img src='https://drive.google.com/uc?export=view&id=1hDlrXcM3uWz2wHVdBIqQ8JLYCv0us4QO')/>
<p>Figure 4. Compare with Upscale CNN x2 + Retore CNN & Real-ESRGAN effect</p>
</div>

### Run Time Comparisons(GPU)
Following comparisons are done with 720P video as input. 

<div align="center">
<img src='https://drive.google.com/uc?export=view&id=1vjPsHItDaexbXRbnOsa2Wop259mlGX54')/>
<p>Figure 5. GPU time for different effects (720P)</p>
</div>


Following comparisons are done with RTX 3070Ti graphic card. 

<div align="center">
<img src='https://drive.google.com/uc?export=view&id=1fXaNjpD3M4H48xkoekhj__t6M_kYwhEd')/>
<p>Figure 6. GPU time for different effects with different resolution videos</p>
</div>

## Future Improvements

* Use `read-write` storage texture instead of `write-only` storage texture (Not yet supported in Chrome stable) for lower VRAM usage.

## Reference
This project references a variety of resources:
- **Multimedia Demonstrations**
  <!-- - Demo Video: Miss Kobayashi's Dragon Maid: [YouTube Video](https://www.youtube.com/watch?v=NQF3a6A7kcQ) -->
  - Demo Image: Higurashi 360P from Anime4K [(repo)](https://github.com/bloc97/Anime4K/tree/master)

## Credits

* [Anime4K](https://github.com/bloc97/Anime4K)
* [UnityAnime4K](https://github.com/keijiro/UnityAnime4K)
* [WebGPU-Samples](https://github.com/webgpu/webgpu-samples)
